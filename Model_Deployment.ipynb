{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anchit-2002/Phising-Website-Detection-/blob/main/Model_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6imUqOiuhpy",
        "outputId": "d35190b0-ca1c-42c8-9887-c5edb4db0a88",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.11.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.3 (from gradio)\n",
            "  Downloading gradio_client-1.5.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.3->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.3->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.11.0-py3-none-any.whl (57.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.3-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.11.0 gradio-client-1.5.3 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "_C5QCfvyuNum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "with open('/content/rf model', 'rb') as f:\n",
        "    model = pickle.load(f)"
      ],
      "metadata": {
        "id": "TfEQiMrIuYuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title_from_url(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract the title from the <title> tag\n",
        "        title = soup.title.string if soup.title else None\n",
        "\n",
        "        # Return the decoded title or a placeholder if it's None\n",
        "        return title if title else \"No Title Found\"\n",
        "    else:\n",
        "        return \"Failed to retrieve URL\"\n"
      ],
      "metadata": {
        "id": "0N5hfI9iRmgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for checking empty or generic title\n",
        "def check_empty_or_generic_title(title):\n",
        "\n",
        "    generic_terms = ['home', 'index', 'default', 'welcome', 'about', 'contact', 'services', 'login', 'register']\n",
        "\n",
        "    title = title.lower() if title else ''\n",
        "    if not title or any(term in title for term in generic_terms):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def check_encoding_artifacts(content):\n",
        "    non_ascii = re.findall(r'[^\\x00-\\x7F]', content)\n",
        "    return len(non_ascii) > 0"
      ],
      "metadata": {
        "id": "kW-SeSwLT6DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "# Load the trained model\n",
        "with open('rf model','rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Function to preprocess the input URL\n",
        "def preprocess_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        content = response.text\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "    except Exception:\n",
        "        content = \"\"\n",
        "        soup = None\n",
        "    # Extract features\n",
        "    features = {\n",
        "        'URLLength': len(url),\n",
        "        'DomainLength': len(url.split('/')[2]),\n",
        "        'IsDomainIP': 1 if url.split('/')[2].replace('.', '').isdigit() else 0,\n",
        "        'URLSimilarityIndex': 0.5,  # Placeholder value\n",
        "        'CharContinuationRate': sum(\n",
        "            1 for i in range(len(url) - 1) if url[i].isalnum() and url[i + 1].isalnum()\n",
        "        ) / len(url) if len(url) > 1 else 0,\n",
        "        'TLDLegitimateProb': 0.9,  # Placeholder value\n",
        "        'URLCharProb': 0.8,  # Placeholder value\n",
        "        'TLDLength': len(url.split('/')[2].split('.')[-1]),\n",
        "        'NoOfSubDomain': len(url.split('/')[2].split('.')) - 1,\n",
        "        'HasObfuscation': 1 if re.search(r'(0x|%[0-9A-Fa-f]{2}|[a-z]\\d[a-z])', url) else 0,\n",
        "        'NoOfObfuscatedChar': sum(1 for match in re.findall(r'(0x|%[0-9A-Fa-f]{2}|[a-z]\\d[a-z])', url)),\n",
        "        'ObfuscationRatio': sum(1 for c in url if c not in string.ascii_letters + string.digits) / len(url),\n",
        "        'NoOfLettersInURL': sum(c.isalpha() for c in url),\n",
        "        'LetterRatioInURL': sum(c.isalpha() for c in url) / len(url) if len(url) > 0 else 0,\n",
        "        'NoOfDegitsInURL': sum(c.isdigit() for c in url),\n",
        "        'DegitRatioInURL': sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0,\n",
        "        'NoOfEqualsInURL': url.count('='),\n",
        "        'NoOfQMarkInURL': url.count('?'),\n",
        "        'NoOfAmpersandInURL': url.count('&'),\n",
        "        'NoOfOtherSpecialCharsInURL': sum(\n",
        "            1 for c in url if c not in string.ascii_letters + string.digits + '&' + ':/?=#.'\n",
        "        ),\n",
        "        'SpacialCharRatioInURL': sum(\n",
        "            1 for c in url if not c.isalnum()\n",
        "        ) / len(url) if len(url) > 0 else 0,\n",
        "        'IsHTTPS': 1 if url.startswith(\"https://\") else 0,\n",
        "        'LineOfCode': len(content.splitlines()) if content else 0,\n",
        "        'LargestLineLength': max(len(line) for line in content.splitlines()) if content else 0,\n",
        "        'HasTitle': 1 if soup and soup.title else 0,\n",
        "        'DomainTitleMatchScore': 1 if soup and soup.title and url.split('/')[2] in soup.title.string else 0,\n",
        "        'HasFavicon': 1 if soup and soup.find(\"link\", rel=\"icon\") else 0,\n",
        "        'Robots': 1 if soup and soup.find(\"meta\", attrs={\"name\": \"robots\"}) else 0,\n",
        "        'IsResponsive': 1 if soup and soup.find(\"meta\", attrs={\"name\": \"viewport\"}) else 0,\n",
        "        'NoOfURLRedirect': len(response.history) if response else 0,\n",
        "        'NoOfSelfRedirect': 1 if response and response.history and response.history[-1].url == url else 0,\n",
        "        'HasDescription': 1 if soup and soup.find(\"meta\", attrs={\"name\": \"description\"}) else 0,\n",
        "        'NoOfPopup': content.count(\"window.open\") if content else 0,\n",
        "        'NoOfiFrame': len(soup.find_all(\"iframe\")) if soup else 0,\n",
        "        'HasExternalFormSubmit': 1 if soup and soup.find(\"form\", action=lambda x: x and url.split('/')[2] not in x) else 0,\n",
        "        'HasSocialNet': 1 if soup and any(network in content.lower() for network in [\"facebook\", \"twitter\", \"linkedin\"]) else 0,\n",
        "        'HasSubmitButton': 1 if soup and soup.find(\"button\", type=\"submit\") else 0,\n",
        "        'HasHiddenFields': 1 if soup and soup.find(\"input\", type=\"hidden\") else 0,\n",
        "        'HasPasswordField': 1 if soup and soup.find(\"input\", type=\"password\") else 0,\n",
        "        'Bank': 1 if re.search(r'\\b(bank|banking)\\b', content.lower()) else 0,\n",
        "        'Pay': 1 if re.search(r'\\b(pay|payment)\\b', content.lower()) else 0,\n",
        "        'Crypto': 1 if re.search(r'\\b(crypto|cryptocurrency)\\b', content.lower()) else 0,\n",
        "        'HasCopyrightInfo': 1 if re.search(r'\\b(copyright|©)\\b', content.lower()) else 0,\n",
        "        'NoOfImage': len(soup.find_all(\"img\")) if soup else 0,\n",
        "        'NoOfCSS': len(soup.find_all(\"link\", rel=\"stylesheet\")) if soup else 0,\n",
        "        'NoOfJS': len(soup.find_all(\"script\")) if soup else 0,\n",
        "        'NoOfSelfRef': len(soup.find_all(\"a\", href=lambda x: x and url.split('/')[2] in x)) if soup else 0,\n",
        "        'NoOfEmptyRef': len(soup.find_all(\"a\", href=lambda x: not x or x == \"#\")) if soup else 0,\n",
        "        'NoOfExternalRef': len(soup.find_all(\"a\", href=lambda x: x and url.split('/')[2] not in x)) if soup else 0,\n",
        "        'EncodingArtifacts': 0,  # Placeholder: Replace with actual encoding check function\n",
        "        'IsEmptyOrGenericTitle': 0,  # Placeholder: Replace with actual title check function\n",
        "    }\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "anwfG1t_xslW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "with open(\"pca\", \"rb\") as f:\n",
        "    pca = pickle.load(f)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "def predict_url(url):\n",
        "    try:\n",
        "        # Preprocess the URL to extract features\n",
        "        features = preprocess_url(url)\n",
        "        features_array = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "        # Standardize and apply PCA using the preloaded scaler and PCA\n",
        "        features_scaled = scaler.fit_transform(features_array)  # Use preloaded scaler\n",
        "        features_pca = pca.transform(features_scaled)       # Use preloaded PCA\n",
        "\n",
        "        # Make a prediction using the trained model\n",
        "        prediction = model.predict(features_pca)\n",
        "\n",
        "        # Map the prediction to phishing or legitimate\n",
        "        if prediction == 0:\n",
        "            return \"Phishing\"\n",
        "        if prediction ==1:\n",
        "            return \"Legitimate\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle errors during preprocessing or prediction\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nKKfNgcqUw1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_url('\thttp://rapidfsf.com')"
      ],
      "metadata": {
        "id": "GlGhYmFjVbAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pOdKKVm8UwkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn= predict_url,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Phishing URL Classifier\",\n",
        "    description=\"Enter a URL to check if it's phishing or legitimate.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "AnrtPtz9a1q5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "7a398dcd-5ef4-4d97-f05d-245f848a896a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9dc7e438205d1c0a6e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9dc7e438205d1c0a6e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}